Hive 元数据库
提供类似SQL的语法,转换为mapreduce程序执行

hive 将元数据存储在数据库中(metastore),支持mysql,derby等数据库。
hive 中的元数据包括表的名字,表的列和分区及其属性,表的属性,表的数据所在的目录等.

grant all privileges on *.* to 'root'@'%' identified by 'admin' with grant option;
flush privileges;

hive中通常一个数据表对应一个文件
hdfs:/hive/student
hdfs:/hive/score

HQL的执行过程
解释器，编译器，优化器 完成HQL查询语句从词法分析，语法分析，编译，优化 以及查询计划的生成
生成的查询计划存储在hdfs 中，并在随后有mapreduce调用执行。

HQL -- 解释器(词法分析) --- 编译器(生成HQL的执行计划)--- 优化器(生成最佳的执行计划)---执行
sqlplus c##liuxun/123456@127.0.0.1:1521/orcl

CLI(Command Line Interface)  JDBC/ODBC       Web Console控制台 
                          （Thrift Server）

                        Hiver Driver
archive.apache.org Apache的所有项目的下载目录

Hive 的安装模式

1.嵌入模式 
  元数据信息存储在hive自带的Derby数据库中
  只能创建一个连接

  在指定的目录解压hive(root目录下) tar -zxvf apache-hive-0.13.0-bin.tar.gz
  添加路径 HIVE_HOME=/root/apache-hive-0.13.0-bin
  export PATH=$PATH:$HIVE_HOME/bin
  执行hive 

   source
2.本地模式
   使用mysql 数据库，并和hive安装在同一台物理主机

3.远程模式
   mysql 安装在另一台机器上

   二台虚拟机 mysql安装在 windows 系统中 创建hive数据库
   解压hive ... 将mysql的驱动jar 放入 hive解压目录的lib目录中
   修改hive-site.xml （参考hive-default.xml.template）
   可参考apache官网 hive.apache.org -->Hive wiki -->Administrator Documentdoc -->Setting up metastore
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
   <property>
      <name>javax.jdo.option.ConnectionURL</name>
      <value>jdbc:mysql://192.168.1.102:3306/hive</value>
   </property>
   <property>
      <name>javax.jdo.option.ConnectionDriverName</name>
      <value>com.mysql.jdbc.Driver</value>
   </property>
   <property>
      <name>javax.jdo.option.ConnectionUserName</name>
      <value>root</value>
   </property>
   <property>
      <name>javax.jdo.option.ConnectionPassword</name>
      <value></value>
   </property>
   <property>
  <name>hive.hwi.listen.host</name>
  <value>0.0.0.0</value>
   <description>This is the host address the Hive Web Interface will listen on</description>
  </property>
    <property>
        <name>hive.hwi.listen.port</name>
	  <value>9999</value>
	  <description>This is the port the Hive Web Interface will listen on</description>
     </property> 
	<property>
	  <name>hive.hwi.war.file</name>
	  <value>${HIVE_HOME}/lib/hive-hwi-<version>.war</value>
	  <description>This is the WAR file with the jsp content for Hive Web Interface</description>
	</property>
</configuration>
四个参数分别为 jdbcurl , drivername , username ,password
还要设置mysql 可以远程连接
create table test1(tid int,tname string); 可查看mysql数据库
 
hive 的管理 
quit exit 


常用的CLI命令
show tabls --注释 
show functions -- 查看函数
desc 表名
dfs -ls 目录 
dfs -lsr 目录
! linux 命令 !pwd;
source xxx.sql 
select * from test1; 不会转换为mapredure任务
1 tom
2 mary
3 mike
select tname from test1 会

hive -S hive 静默模式 不打印 任务信息(linux模式下登陆)
hive - e 'show tables'; 不进入hive直接执行hive命令 

hive --service hwi 

tar -zxvg apache-hive-0.13.0-src.tar.gz
cd apache-hive-0.13.0-src
cd hwi/
jar cvfM0 hive-hwi-0.13.0.war -C web/ .
cp hive-hwi-0.13.0.war /root/apache-hive-0.13.0-bin/lib 注意是hive的安装目录
cd root/apache-hive-0.13.0-bin/conf/
vim hive-site.xml
添加

访问 http://192.168.232.131:9999/hwi/
cp /usr/java/jdk1.8.0_91/lib/tools.jar /root/apache-hive-0.13.0-bin/lib
hive --service hwi
实例
create session 
创建查询语句

--------
hive的java客户端操作
启动hive远程服务
hive --service hiveserver 

GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' IDENTIFIED BY '' WITH GRANT OPTION;

------------------------------
CLI 命令行的方式
$HIVE_HOME/bin/hive的执行程序 
hive --serviec cli

hive 远程服务启动方式 (以JDBC或ODBC的程序登陆到hive)
端口号 10000
启动方式 hive --service hiveserver

----
create table person
( pid int,
  pname string,
  married boolean,
  salary double
);
desc person;
====
hive 中的表 
table 内部表
partition 分区表
external table 外部表 指向已经在HDFS中存在的数据，可以创建partition
bucket table 桶表 桶表对数据进行哈希取值 然后分别存储。
视图 虚表 逻辑概念 可以跨越多张表

create table t1
(tid int,tname string,age int);
create or replace table t1(id int,name string)
row format delimited fields terminated by ',';
-
create table t2
(tid int,tname string,age int)
row format delimited fields terminated by ',';
location '/root/hivetest/t2.txt';
-
create table t3
(tid int,tname string,age int);
row format delimited fields terminated by ',';
-
create table t4
as 
select * from sample_data;
-
create table t5
row format delimited fields terminated by ','
as
select * from sample_data;
-
create table partition_table 分区表
(sid int,sname string)
partitioned by (gender string)
row format delimited fields terminated by ',';
-
create table bucket_table
(sid int,sname string,age int)
clustered by(sname) into 5 buckets; 桶表
-


create view empinfo
as
select e.empno,e.ename,e.sal d.dname
from emp e,dept d
where e.deptno=d.deptno; 视图

-------------外部表实例---------
1	Aom	M
2	Bom	M
3	Com	F
4	Dom	F

5	Eike	F
6	Fco	M
7	Gie	F

8	Xom	M
9	Yck	F
10	Zilo	M

1. 创建 student1.txt student2.txt student3.txt 存入数据 
2. 放入hadoop上  hdfs dfs -put /root/hivetest/student1.txt /hive1
                 hdfs dfs -put /root/hivetest/student2.txt /hive1
		 hdfs dfs -put /root/hivetest/student3.txt /hive1
3.创建外部表
create external table external_student
(sid int,sname string,sex string)
row format delimited fields terminated by ','
location '/hive1';

select * from external_student;
-------------外部表实例---------

-------------------分区表实例----------------
create table partition_table1 
(sid int,sname string)
partitioned by (sex string)
row format delimited fields terminated by ','
location '/hive2';

insert into table partition_table1 partition(sex='M',sex='F')
select sid,sname from external_student;   程序不对。。。。。。

-------------------分区表实例----------------

--------------------hive 的数据操作 和开发 -----------------------
hive 不支持insert 语句
使用load语句 

create table t2
(tid int,tname string,age int)
row format delimited fields terminated by ',';
location '/hive1';

21,sha,210
22,haha,220
23,xixi,230

load data local inpath '/root/hivetest/t2.txt' into table t2;
--------------------hive 的数据操作 和开发 -----------------------
使用load语句
LOAD DATE [LOCAL] INPATH 'linuxfilepath' [OVERWRITE]
INTO TABLE tablename [PARTITION (partition1=val1,...)]

overwrite 表示删除原有数据
将/root/hive1/t3.txt 文件导入t3表中
load data local inpath '/root/hive1/t3.txt' into t3;

将/root/hive1 下的所有数据文件导入t3表中，并覆盖原来的数据
load data local inpath '/root/hive1/' overwrite into t3;

----------------------sqoop的使用----------------------------
sqoop 
使用sqoop 导入oracle 数据到 HDFS

sqoop import --connect jdbc:oracle:thin:@14.120.29.183:1521:orcl --username c##scott --password tiger --table emp --columns 'empno,ename,job,sal,deptno' -m 1 --target-dir '/sqoop/emp'
终于成功执行了(重启就成功了)
-
sqoop list-tables --connect jdbc:oracle:thin:@14.120.29.183:1521:orcl --username c##scott --password=tiger
成功执行。
-
使用sqoop 导入oracle 数据到 hive
sqoop import --hive-import --connect jdbc:oracle:thin:@14.120.29.183:1521:orcl --username c##scott --password tiger --table emp -m 1 --columns 'empno,ename,job,sal,deptno';
-
使用sqoop 导入oracle 数据到 hive ,并指定表名
sqoop import --hive-import --connect jdbc:oracle:thin:@14.120.29.183:1521:orcl --username c##scott --password tiger -m 1 --table EMP
好奇怪，当我表名小写并指定了列的时候  只导入到了HDFS ，当我表名小写又不指定列出错，表名大写就好了。
-
使用sqoop 导入oracle 数据到 hive中 使用where条件
sqoop import --hive-import --connect jdbc:oracle:thin:@192.168.1.102:1521:orcl
--username c##scott --password tiger --table emp -m 1 --columns 'empno,ename,job,sal,deptno'
--hive-table emp2 --where 'deptno=10'
-
使用sqoop 导入oracle 数据到 hive中 使用查询语句
sqoop import --hive-import --connect jdbc:oracle:thin:@192.168.1.102:1521:orcl
--username c##scott --password tiger --table emp -m 1 --query 'select * from emp where sal<2000 and $CONDITIONS'
--targer-dir '/sqoop/emp5' --hive-table emp5
-
使用sqoop将hive中的数据导出到Oracle 中
sqoop export --connect jdbc:oracle:thin:@192.168.1.102:1521:orcl 
--username c##scott --password tiger -m 1 --table myemp --export-dir ******

----------------------sqoop的使用----------------------------
sqoop help 查看帮助文档
sqoop help COMMAND 查看命令的详细使用方法

hive 的数据查询
简单查询
过滤和排序
hive 的函数

语法
select [all | distinct] select_expr,select_expr,...
from table_referrence
[where where_condition]
[group by col_list]
[cluster by col_list 
             | [distribute by col_list] [sort by col_list] 
	     | [order by col_list]]
[limit number]

distribute by 指定分发器 多reducer可用

select * from emp;
select empno,ename,sal from emp;
select empno,ename,sal,sal*12 from emp;
select empno,ename,sal,sal*12,comm,sal*12+nvl(comm,0) from emp;
select * from emp where comm is null;
select distinct depno,job from emp;
select * from emp where deptno=10;
select * from emp where ename='KING';
select * from emp where deptno=10 and sal<2000;
select * from emp where ename like 'S%';
select * from emp where ename like '%\\_%';
select empno,ename,sal from emp order by sal desc;

set hive.fetch.task.conversion=more;
hive --hiveconf hive.fetch.task.conversion=more;
修改hive-site.xml文件

数学函数
round
ceil
floor
select round(45.926,2),round(45.926,1),round(45.926,0),round(45.926,-1),round(45.926,-2),round(45.926,1);

字符函数
lower
upper
length
concat
substr
trim
lpad
rpad

select lower('Hello World'),upper('Hello World');
select length('Hello World'),length('你好');
select concat('cao','nimei');d
substr(a,b) 
substr(a,b,c) 从a中的第b为开始取 取c个字符 从1开始
select trim('  sdfs  ');
select lpad('adbc',10,'*'),rpad('adbc',10,'*')
select size(map(1,'tom',2,'mary'));
select cast(1 as bigint);
select cast(1 as float);
select cast('2016-04-10' as date);

日期函数
to_date
year
month
day
weekofyear
datediff
date_add
date_sub

select to_date('2015-04-23 11:23:11');
select year('2015-04-23 11:23:11'),month('2015-04-23 11:23:11'),day('2015-04-23 11:23:11'),weekofyear('2015-04-23 11:23:11');
select datediff('2015-04-23 11:23:11','2016-04-23 11:23:11');
select date_add('2015-04-23 11:23:11',2),date_sub('2015-04-23 11:23:11',2)

coalesce 从左到右返回第一个不为null的值
select comm,sal,coalesce(comm,sal) from emp;

select ename,job,sal
       case job when 'PRESIDENT' then sal+=1000
                when 'MANAGER' then sal+=500
		else sal+400 end
from emp;
聚合函数
count
sum
min
max
avg

select count(*),sum(sal),max(sal),min(sal),avg(sal) from emp;

select explode(map(1,'Tom',2,'Mary',3,'Mike'));

hive的表连接
等值连接
不等值连接
外连接
自连接
desc dept
desc emp;

desc salgrade;
select e.empno,e.ename,e.sal,s.grade
from emp e,salagrade s
where e.sal between s.losal and s.hisal;

select e.empno,e.ename,e.deptno,d.name
from emp e,dept d 
where e.deptno=d.deptno 

select d.deptno,d.dname,count(e.empno)
from emp e,dept d
where e.deptno=d.deptno
group by d.deptno,d.dname

select d.deptno,d.dname,count(e.empno)
from emp e right outer join dept d
on (e.deptno=d.deptno)
group by d.deptno,d.dname;

自连接
select e.ename,b.ename
from emp e,emp b
where e.mgr=b.empno;



inpath 为目录时导入目录下的所有数据
题1 导入数据到表  题2 导入数据到分区表

export-dir hdfs目录



使用命令强制离开
hadoop dfsadmin -safemode leave

hive 只支持 from和where子句中的子查询
select e.ename from emp e where e.deptno in (select d.deptno from dept d where d.name='SALES' or d.dname='ACCOUNTING')

-----
Hive自定义函数的实现
自定义UDF需要继承org.apache.hadoop.hive.ql.UDF
需要实现evaluate函数, evaluate函数支持重载

Hive 自定义函数的部署运行

把程序打包放到目标机器上去
进入hive客户端，添加jar包
hive> add jar /root/hive目录/udfjar/xxx.jar
创建临时函数
hive> create temporary function myconcat as 'com.lx.udf.ConcatString'
select myconcat('hello','world');
销毁临时函数
drop tmporary function 函数名

--hive插入数据
insert overwrite table xxx.yyy select 100,'helloword' from default.dual;

----------------------------
show tables/partitions/indexes/databases/columns/functions
显示所有表
show partitions tablename --显示所有分区
显示表的扩展信息
显示数据库
显示所有列
显示所有函数
show granted roles and privileges 显示所有角色与权限
show locks 显示锁
show conf 显示配置文件
show transactions 显示事务
show schemas;
show databases like '*dm*';
show tables like '...xxx';  例如adcxxx
desc database databasename; 
desc database extended databasename; 
--------------------
hive命令
进入与退出hive交互 hive quit exit
参数设置 set reset
资源文件管理 add list delete
执行shell命令 !cmd
hdfs文件操作 dfs-ls dfs-cat
hiveQL

